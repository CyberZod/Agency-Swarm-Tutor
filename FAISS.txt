Thatâ€™s a solid plan! Here's how the QA Agent would work in your Agency Swarm setup:

ðŸ”¹ Workflow Overview
ScraperAgent scrapes a website and stores the file paths in shared state. âœ… (Already Done)

QA Agent takes these markdown files, processes them, and uploads them to a vector database.

When a user asks a question, the QA Agent retrieves relevant context from the vector DB and answers using an LLM.

ðŸ”¹ Tech Stack Choices
To implement this, weâ€™ll need:

FAISS (Facebook AI Similarity Search) â†’ for fast vector-based retrieval.

LangChain â†’ for LLM-based question answering.

OpenAI Embeddings API â†’ to convert text into vectors.

ðŸ”¹ How the QA Agent Works
1ï¸âƒ£ Upload Scraped Data to the Vector Store
Reads the .md files from shared state.

Converts them into vector embeddings using OpenAI.

Stores them in FAISS for fast retrieval.

2ï¸âƒ£ Answer User Questions
Converts the user's question into a vector.

Finds the most relevant context from FAISS.

Passes the context + question to an LLM to generate the answer.

âœ… Implementing the QA Agent Tool
python
Copy
Edit
import os
import faiss
import pickle
from typing import List
from agency_swarm.tools import BaseTool
from pydantic import Field
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.llms import OpenAI
from langchain.chains import RetrievalQA

VECTOR_STORE_PATH = "vector_store"  # Directory to save FAISS index
os.makedirs(VECTOR_STORE_PATH, exist_ok=True)

class QATool(BaseTool):
    """
    A tool that indexes scraped website content into a vector store 
    and answers questions using a language model.
    """

    question: str = Field(
        ..., description="The user question to answer based on scraped website data."
    )

    def run(self) -> str:
        """
        Handles user questions by retrieving relevant information from the vector store 
        and generating an answer.

        Returns:
            str: The answer generated by the LLM.
        """
        faiss_index = self.load_or_build_vector_store()
        return self.answer_question(faiss_index, self.question)

    def load_or_build_vector_store(self) -> FAISS:
        """Loads the FAISS vector store or builds it if it doesn't exist."""
        index_path = os.path.join(VECTOR_STORE_PATH, "faiss_index")
        data_path = os.path.join(VECTOR_STORE_PATH, "documents.pkl")

        # If FAISS index exists, load it
        if os.path.exists(index_path) and os.path.exists(data_path):
            with open(data_path, "rb") as f:
                documents = pickle.load(f)
            faiss_index = FAISS.load_local(index_path, OpenAIEmbeddings())
            print("Loaded existing vector store.")
            return faiss_index

        # Otherwise, build a new index from the scraped files
        file_paths = self._shared_state.get("scraped_files:https://ai.pydantic.dev")
        if not file_paths:
            return "No scraped data found in shared state."

        print(f"Processing {len(file_paths)} files into vector store...")
        documents = []
        for file_path in file_paths:
            with open(file_path, "r", encoding="utf-8") as f:
                documents.append(f.read())

        # Split text into chunks for better retrieval
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        chunks = text_splitter.split_text("\n\n".join(documents))

        # Convert text chunks into vector embeddings
        faiss_index = FAISS.from_texts(chunks, OpenAIEmbeddings())

        # Save FAISS index and documents
        faiss_index.save_local(index_path)
        with open(data_path, "wb") as f:
            pickle.dump(documents, f)

        print("Vector store built and saved.")
        return faiss_index

    def answer_question(self, faiss_index: FAISS, question: str) -> str:
        """Uses the vector store and LLM to answer a user question."""
        llm = OpenAI(model="gpt-4-turbo")
        retriever = faiss_index.as_retriever()
        qa_chain = RetrievalQA(llm=llm, retriever=retriever)
        answer = qa_chain.run(question)

        return f"**Q:** {question}\n**A:** {answer}"


if __name__ == "__main__":
    tool = QATool(question="What is PydanticAI?")
    print(tool.run())
ðŸ”¹ Breakdown of What This Does